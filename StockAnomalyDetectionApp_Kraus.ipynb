# Stock Anomaly Detection Application (v1.Beta) - NVIDIA Developer and LlamaIndex Contest 2024 Submission - By Jamie Kraus Updated 11.10.2024

################################################## PROJECT CODE STARTS ##############################################################
# Includes all necessary imports for various functionalities such as handling PDFs, plotting, and interacting with the Qdrant vector database and NVIDIA API.
# Initializes the NVIDIA API client and sets the API key.
# Defines a class to handle vector storage and search operations using Qdrant, including methods for inserting and searching vectors.
# Provides a helper function to get completions from the NVIDIA Nemotron model.

import os
import numpy as np
import pdfplumber
import gradio as gr
from pydantic import BaseModel, Field
from qdrant_client import QdrantClient
from qdrant_client.http import models as rest
import traceback
import requests
import atexit
import yfinance as yf
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
from scipy.signal import find_peaks
import pandas as pd
import uuid
from openai import OpenAI
import openai
import time
import re
import matplotlib.dates as mdates

# Corrected imports from LlamaIndex based on confirmed paths
from llama_index.core import VectorStoreIndex, ServiceContext, Document
from llama_index.embeddings.nvidia import NVIDIAEmbedding
from llama_index.llms.nvidia import NVIDIA
from llama_index.core.node_parser import SentenceSplitter

class CustomQdrantVectorStore:
    def __init__(self, url, port, collection_name, vector_size, distance, overwrite=False):
        self.client = QdrantClient(url=url, port=port)
        self.collection_name = collection_name

        # Check if the collection already exists
        try:
            if self.client.get_collection(collection_name=self.collection_name):
                if overwrite:
                    self.client.delete_collection(collection_name=self.collection_name)
                    self.client.create_collection(
                        collection_name=self.collection_name,
                        vectors_config=rest.VectorParams(
                            size=vector_size,
                            distance=distance
                        )
                    )
        except Exception as e:
            print(f"Error handling collection: {str(e)}")

    def insert(self, vectors):
        try:
            payload = [{'id': str(uuid.uuid4()), 'vector': vec.tolist()} for vec in vectors]
            self.client.upsert(collection_name=self.collection_name, points=payload)
        except Exception as e:
            print(f"Error inserting vectors: {str(e)}")

    def search(self, vector, top_k):
        try:
            results = self.client.search(
                collection_name=self.collection_name,
                query_vector=vector.tolist(),
                limit=top_k
            )
            return results
        except Exception as e:
            print(f"Error during search: {str(e)}")
            return []

# Initialize NVIDIA API client
client = OpenAI(
    api_key="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",
    base_url="https://integrate.api.nvidia.com/v1"
)

# Function to get a completion from NVIDIA Nemotron
def get_completion(system_content, user_content):
    completion = client.chat.completions.create(
        model="nvidia/llama-3.1-nemotron-51b-instruct",
        messages=[
            {"role": "system", "content": system_content},
            {"role": "user", "content": user_content}
        ],
        temperature=0.5,
        top_p=1,
        max_tokens=512,
        stream=False
    )
    return completion.choices[0].message.content  # Adjusted to match the expected response format

# Ensure NVIDIA API key is set
os.environ["NVIDIA_API_KEY"] = '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'

#Defines the CustomDocument class for handling document data.
#Implements utility functions for extracting text from PDFs, getting file names, and embedding texts using the NVIDIA model.

# Define CustomDocument class
class CustomDocument(BaseModel):
    text: str
    id_: str
    unique_hash: int

    @staticmethod
    def create(text: str, id_: str):
        unique_hash = hash(text)
        return CustomDocument(text=text, id_=id_, unique_hash=unique_hash)

# Extract text from PDF
def extract_text_from_pdf(file_path):
    try:
        with pdfplumber.open(file_path) as pdf:
            text = ""
            for page in pdf.pages:
                text += page.extract_text()
        return text
    except Exception as e:
        print(f"Error extracting text from file {file_path}: {e}")
        return None

# Function to get file names from file objects
def get_files_from_input(file_objs):
    if not file_objs:
        return []
    return [file_obj.name for file_obj in file_objs]

# Function to embed texts using NVIDIA model
def embed_texts_with_nvidia(texts):
    embeddings = []
    for text in texts:
        if len(text) > 511:
            text = text[:511]  # Truncate the text to stay within the maximum allowed token size
        response = client.embeddings.create(
            input=[text],
            model="nvidia/nv-embedqa-mistral-7b-v2",
            encoding_format="float",
            extra_body={"input_type": "query", "truncate": "NONE"}
        )
        # Truncate the embedding to the first 256 dimensions to fit Qdrant's expected dimension
        truncated_embedding = response.data[0].embedding[:256]
        embeddings.append(truncated_embedding)
    return np.array(embeddings)

# Defines the DocumentLoader class for handling the loading of documents,
# checking for duplicates, extracting text, and embedding texts using the CustomQdrantVectorStore

class DocumentLoader:
    def __init__(self):
        self.loaded_documents = set()
        self.qdrant_url = "http://localhost"
        self.collection_name = "qdrant-server"
        self.vector_store = CustomQdrantVectorStore(
            url=self.qdrant_url,
            port=6333,
            collection_name=self.collection_name,
            vector_size=256,
            distance='Cosine',
            overwrite=False
        )

    def load_documents(self, file_input):
        if file_input is None:
            return "No documents selected.", self.generate_document_table()
        
        existing_titles = {os.path.basename(doc) for doc in self.loaded_documents}
        load_status = ""
        documents = []
        duplicates_found = False
        
        for file in file_input:
            if os.path.basename(file.name) in existing_titles:
                duplicates_found = True
                continue
            text = extract_text_from_pdf(file.name)
            if text is None:
                return f"Error extracting text from file {file.name}.", self.generate_document_table()
            doc = CustomDocument.create(text=text, id_=file.name)
            documents.append(doc)
            full_file_path = os.path.abspath(file.name)
            self.loaded_documents.add(full_file_path)
        
        if duplicates_found:
            return "Already uploaded PDF(s), please upload a new PDF.", self.generate_document_table()

        if not documents:
            return "No documents found in the selected files.", self.generate_document_table()

        texts = [doc.text for doc in documents]
        embeddings = embed_texts_with_nvidia_parallel(texts)
        points = [
            rest.PointStruct(
                id=str(uuid.uuid4()),
                vector=embedding.tolist(),
                payload={"file_path": full_file_path, "doc_id": doc.id_}
            )
            for doc, embedding, file in zip(documents, embeddings, file_input)
        ]
        
        self.vector_store.client.upsert(
            collection_name=self.collection_name,
            points=points
        )
        load_status = f"Successfully loaded {len(documents)} documents."
        return load_status, self.generate_document_table()
    
    def generate_document_table(self):
        table_html = """
        <div style='height: 300px; overflow-y: auto; border: 1px solid #ccc; padding: 10px;'>
            <table style='table-layout: fixed; width: 100%;'>
                <tr>
                    <th style='text-align: center;width: 50px;'>No.</th>
                    <th style='text-align: center;width: 92px;'>File Directory</th>
                    <th style='text-align: center;width: 92px;'>PDF Title</th>
                    <th style='text-align: center;width: 60px;'>Pages</th>
                </tr>
        """
        total_documents = len(self.loaded_documents)
        for index, doc_path in enumerate(self.loaded_documents, 1):
            pdf_title = os.path.basename(doc_path)
            short_doc_path = (doc_path if len(doc_path) <= 15 else doc_path[:15] + '... <button onclick="this.previousSibling.textContent=\'' + doc_path + '\'; this.textContent=\'See Less\'; this.onclick=function(){this.previousSibling.textContent=\'' + doc_path[:15] + '... \'; this.textContent=\'See More\';}">See More</button>')
            try:
                with pdfplumber.open(doc_path) as pdf:
                    num_pages = len(pdf.pages)
                table_html += f"<tr><td style='text-align: center; width: 30px;'>{index}</td><td style='word-wrap: break-word; overflow-x: auto; text-align: center;'>{short_doc_path}</td><td style='word-wrap: break-word; overflow-x: auto; text-align: center;'>{pdf_title}</td><td style='word-wrap: break-word; overflow-x: auto; text-align: center;'>{num_pages}</td></tr>"
            except Exception as e:
                print(f"Error processing PDF {pdf_title}: {e}")
                table_html += f"<tr><td style='text-align: center; width: 30px;'>{index}</td><td style='word-wrap: break-word; overflow-x: auto; text-align: center;'>{short_doc_path}</td><td style='word-wrap: break-word; overflow-x: auto; text-align: center;'>{pdf_title}</td><td style='word-wrap: break-word; overflow-x: auto; text-align: center;'>Error</td></tr>"
        table_html += "</table></div>"
        return table_html + f"<p style='text-align: right;'>Total Documents: {total_documents}</p>"
    
    def clear_loaded_documents(self):
        self.loaded_documents.clear()
        return "Cleared loaded documents.", self.generate_document_table()

document_loader = DocumentLoader()

# Implements the chat function for handling chat interactions using the CustomQdrantVectorStore.
# Implements the stream_response function for streaming responses, combining LLM responses with document information from Qdrant.

# Function to summarize text using NVIDIA Nemotron LLM
def summarize_text(text):
    system_prompt = "Please summarize the following text briefly and concisely answering the user prompt question and be conversational:"
    user_prompt = text
    summary = get_completion(system_prompt, user_prompt)
    return summary

# Function to extract relevant snippets from PDF based on search results
def extract_relevant_snippets(query_text):
    search_result = document_loader.vector_store.client.search(
        collection_name=document_loader.collection_name,
        query_vector=embed_texts_with_nvidia([query_text])[0].tolist(),  # Embed the query text
        limit=5
    )
    snippets = []
    processed_files = set()
    for result in search_result:
        doc_id = result.payload["doc_id"]
        if doc_id in processed_files:
            continue  # Skip duplicate files
        text = extract_text_from_pdf(doc_id)
        # Use only relevant segments of the text
        snippet_start = text.lower().find(query_text.lower())
        if snippet_start != -1:
            snippet_end = snippet_start + min(500, len(text) - snippet_start)  # Extract up to 500 characters around the query
            snippets.append(text[snippet_start:snippet_end])
        else:
            snippets.append(text[:500])  # Fallback to first 500 characters
        processed_files.add(doc_id)
    combined_snippets = " ".join(snippets)
    return combined_snippets

# Function to retrieve and summarize text from the vector database
def retrieve_and_summarize(query_text):
    combined_snippets = extract_relevant_snippets(query_text)
    summary = summarize_text(combined_snippets)
    return summary

# Function to generate responses using NVIDIA Nemotron LLM with inference
def generate_response_with_inference(query, context):
    system_prompt = f"Given the following context from relevant documents, please answer the user's query concisely and correctly. If you don't find an exact match, provide relevant information that might be useful:\n{context}"
    user_prompt = f"{query}"
    response = get_completion(system_prompt, user_prompt)
    return response

# Function to handle chat interactions
def chat(message, history):
    try:
        if not message.strip():
            raise ValueError("The input message is empty or too short")
        
        # Embed the query
        query_embedding = embed_texts_with_nvidia([message])[0]
        if len(query_embedding) != 256:
            raise ValueError(f"Vector dimension error: expected dim: 256, got {len(query_embedding)}")
        
        # Search for similar embeddings in Qdrant
        search_result = document_loader.vector_store.client.search(
            collection_name=document_loader.collection_name,
            query_vector=query_embedding.tolist(),
            limit=5
        )
        
        # Fetch the relevant documents and extract the text
        responses = []
        for result in search_result:
            doc_id = result.payload["doc_id"]
            text = extract_text_from_pdf(doc_id)
            if text:
                responses.append(text)
        
        # Combine the retrieved texts for inference
        combined_text = " ".join(responses)
        if not combined_text.strip():
            combined_text = "You asked: " + message
        
        # Use the LLM to generate an inference based on the retrieved text or fallback message
        completion = client.chat.completions.create(
            model="nvidia/llama-3.1-nemotron-51b-instruct",
            messages=[{"role": "system", "content": combined_text}, {"role": "user", "content": message}],
            temperature=0.5,
            top_p=1,
            max_tokens=512,
            stream=True
        )
        
        response = ""
        for chunk in completion:
            if chunk.choices[0].delta.content is not None:
                response += chunk.choices[0].delta.content
        
        return history + [(message, response)]
    
    except Exception as e:
        error_message = f"Error processing query: {str(e)}\n{traceback.format_exc()}"
        print(error_message)
        return history + [(message, error_message)]

# Function to stream responses
max_retries = 3
retry_delay = 5

def stream_response(message, history):
    for attempt in range(max_retries):
        try:
            if not message.strip():
                raise ValueError("The input message is empty or too short")
            
            # Embed the query
            query_embedding = embed_texts_with_nvidia_parallel([message])[0]
            if len(query_embedding) != 256:
                raise ValueError(f"Vector dimension error: expected dim: 256, got {len(query_embedding)}")
            
            # Search for similar embeddings in Qdrant
            search_result = document_loader.vector_store.client.search(
                collection_name=document_loader.collection_name,
                query_vector=query_embedding.tolist(),
                limit=5
            )
            
            # Fetch the relevant documents and extract the text
            responses = []
            for result in search_result:
                doc_id = result.payload["doc_id"]
                text = extract_text_from_pdf(doc_id)
                if text:
                    responses.append(text)
            
            # Combine the retrieved texts for inference
            combined_text = " ".join(responses[:2])  # Limit to the first 2 documents to speed up processing
            if not combined_text.strip():
                combined_text = "You asked: " + message
            
            # Increase timeout for the API request
            openai.api_timeout = 60  # 1 minute timeout
            
            # Use the LLM to generate an inference based on the retrieved text or fallback message
            completion = client.chat.completions.create(
                model="nvidia/llama-3.1-nemotron-51b-instruct",
                messages=[{"role": "system", "content": combined_text}, {"role": "user", "content": message}],
                temperature=0.5,
                top_p=1,
                max_tokens=512,
                stream=True
            )
            
            partial_response = ""
            for chunk in completion:
                if chunk.choices[0].delta.content is not None:
                    partial_response += chunk.choices[0].delta.content
                    yield history + [(message, partial_response)]
            break  # Exit the retry loop if successful
        
        except openai.APITimeoutError:
            if attempt < max_retries - 1:
                time.sleep(retry_delay)
            else:
                error_message = "Error processing query: Request timed out after multiple attempts."
                print(error_message)
                yield history + [(message, error_message)]
                break
        except Exception as e:
            error_message = f"Error processing query: {str(e)}\n{traceback.format_exc()}"
            print(error_message)
            yield history + [(message, error_message)]
            break

# Functions to fetch stock data and detect anomalies using an Isolation Forest algorithm.
# Functions to plot stock data with detected anomalies and generate an HTML table of anomalies.

# Function to fetch stock data from Yahoo Finance
def fetch_stock_data(symbol):
    stock_data = yf.download(tickers=symbol, period="3mo", interval="1d")  # Fetch stock data for the last 3 months with daily interval
    if stock_data.empty:
        raise ValueError(f"No data found for symbol {symbol}")
    stock_data.reset_index(inplace=True)
    # Flatten the multi-index DataFrame
    stock_data.columns = [' '.join(col).strip() if isinstance(col, tuple) else col for col in stock_data.columns]
    # Rename columns dynamically
    stock_data.columns = [col.replace(f" {symbol}", "") for col in stock_data.columns]
    print(f"Fetched data: \n{stock_data.tail()}")  # Debugging information
    return stock_data

# Function to detect anomalies in the stock data using Isolation Forest
def detect_anomalies(stock_data):
    if 'Close' not in stock_data.columns or stock_data.empty:
        raise ValueError("Stock data is not in the expected format or is empty.")
    
    isolation_forest = IsolationForest(n_estimators=500, max_samples=0.95, contamination=0.45, random_state=42)
    stock_data['Anomaly'] = isolation_forest.fit_predict(stock_data[['Close']])
    stock_data['Anomaly'] = stock_data['Anomaly'].astype(int)
    print(f"Detected anomalies: \n{stock_data[['Date', 'Close', 'Anomaly']].tail()}")  # Debugging information
    
    # Ensure the first and last rows are not marked as anomalies
    if 'Anomaly' in stock_data.columns and not stock_data.empty:
        stock_data.at[stock_data.index[0], 'Anomaly'] = 1 if stock_data.at[stock_data.index[0], 'Anomaly'] == -1 else stock_data.at[stock_data.index[0], 'Anomaly']
        stock_data.at[stock_data.index[-1], 'Anomaly'] = 1 if stock_data.at[stock_data.index[-1], 'Anomaly'] == -1 else stock_data.at[stock_data.index[-1], 'Anomaly']
    
    return stock_data

# Function to fetch and plot stock data with detected anomalies
def fetch_and_plot_stock_data(stock_symbol):
    try:
        stock_data = fetch_stock_data(stock_symbol)  # Fetch the stock data
        stock_data = detect_anomalies(stock_data)  # Detect anomalies in the stock data
        
        fig, ax = plt.subplots(figsize=(12, 6))
        ax.plot(pd.to_datetime(stock_data['Date']), stock_data['Close'], label='Stock Price')  # Plot the stock price
        
        anomalies = stock_data[stock_data['Anomaly'] == -1]
        print(f"Anomalies data: \n{anomalies}")  # Debugging information
        
        peaks, _ = find_peaks(stock_data['Close'].values)  # Find peaks in the stock price
        valleys, _ = find_peaks(-stock_data['Close'].values)  # Find valleys in the stock price
        
        # Plot anomalies, peaks, and valleys
        ax.scatter(pd.to_datetime(anomalies['Date']), anomalies['Close'], color='red', label='Anomalies')
        ax.scatter(pd.to_datetime(stock_data.iloc[peaks]['Date']), stock_data.iloc[peaks]['Close'], marker='x', color='green', label='Peaks')
        ax.scatter(pd.to_datetime(stock_data.iloc[valleys]['Date']), stock_data.iloc[valleys]['Close'], marker='x', color='purple', label='Valleys')
        
        ax.legend()
        ax.set_title(f'Stock Price and Detected Anomalies for {stock_symbol}')
        ax.set_xlabel('Date')
        ax.set_ylabel('Price')
        plt.xticks(rotation=45)
        plt.tight_layout()
        
        return fig, generate_anomaly_table(stock_data, stock_symbol)  # Return the plot and anomaly table
    except Exception as e:
        print(f"Error fetching or plotting stock data: {str(e)}")
        return None, f"Error: {str(e)}"

# Function to load index data from Yahoo Finance
def load_index_data(index_symbol):
    data = yf.download(index_symbol, period="1y")  # Load one year's data for safety
    data.reset_index(inplace=True)  # Ensure the date is included in the DataFrame
    return data

# Function to get comparative analysis with major indices
def get_comparative_analysis(stock_data, date_str, index_symbols):
    """Perform comparative analysis with major indices."""
    
    stock_change = calculate_price_change(stock_data, pd.to_datetime(date_str))  # Calculate price change for the stock
    analysis_results = []
    
    for index_name, symbol in index_symbols.items():
        index_data = load_index_data(symbol)  # Load index data
        index_change = calculate_price_change(index_data, pd.to_datetime(date_str))  # Calculate price change for the index
        
        if np.isnan(stock_change) or np.isnan(index_change):
            analysis_result = f"{index_name}: N/A"
        else:
            analysis_result = f"{index_name}: {(((stock_change - index_change) / index_change) * 100):.2f}%"  # Comparative performance
        analysis_results.append(analysis_result)
    
    return f"Stock: {stock_change if not np.isnan(stock_change) else 'N/A'}% " + " ".join(analysis_results)

# Updated function for calculate_price_change to handle missing dates
def calculate_price_change(data, date):
    try:
        if date not in data['Date'].values:
            return float('nan')  # Return NaN if the date is not in the data
        idx = data.index[data['Date'] == date].tolist()[0]
        if idx == 0:
            return 0.0  # Return 0.0 if the date is the first entry in the data
        prev_close = data.iloc[idx - 1]['Close']  # Get the previous day's closing price
        curr_close = data.iloc[idx]['Close']  # Get the current day's closing price
        price_change = ((curr_close - prev_close) / prev_close) * 100  # Calculate the percentage price change
        return round(price_change, 4)  # Return the price change rounded to 4 decimal places
    except IndexError:
        print(f"Error: Date {date} not found in data or data is not properly indexed.")  # Handle errors if the date is not found
        return float('nan')  # Return NaN in case of an error

# Function to generate an HTML table of anomalies with comparative performance
def generate_anomaly_table(stock_data, stock_symbol):
    # Convert 'Date' column to datetime without timezone
    stock_data['Date'] = pd.to_datetime(stock_data['Date']).dt.tz_localize(None)
    
    # Load S&P 500 data for comparison
    sp500_data = yf.download('^GSPC', period="1y")[['Close']]
    sp500_data.reset_index(inplace=True)
    sp500_data['Date'] = pd.to_datetime(sp500_data['Date']).dt.tz_localize(None)
    sp500_data.sort_values(by='Date', inplace=True)  # Ensure the data is sorted by date

    # Create HTML table for displaying anomalies and comparisons
    table_html = f"""
    <div style='height: 300px; overflow-y: auto; border: 1px solid #ccc; padding: 10px;'>
        <table style='table-layout: fixed; width: 100%;'>
            <tr>
                <th style='text-align: center;width: 100px;'>{stock_symbol} Anomaly Detected Date</th>
                <th style='text-align: center;width: 70px;'>{stock_symbol} Close Price (USD)</th>
                <th style='text-align: center;width: 70px;'>{stock_symbol} Volume Change (%)</th>
                <th style='text-align: center;width: 70px;'>{stock_symbol} Price Change (%)</th>
                <th style='text-align: center;width: 90px;'>S&P 500 (^GSPC) Price Change (%)</th>
                <th style='text-align: center;width: 100px;'>Comparative Performance to ^GSPC (%)</th>
            </tr>
    """
    
    # Extract anomalies and generate comparisons
    anomalies = stock_data[stock_data['Anomaly'] == -1]

    for i, anomaly in enumerate(anomalies['Date']):
        date_str = pd.to_datetime(anomaly).strftime("%m-%d-%Y")  # Format date as MM-DD-YYYY
        anomaly_date = pd.to_datetime(anomaly)
        
        # Retrieve anomaly volume traded and calculate volume change
        try:
            anomaly_volume_traded = stock_data.loc[stock_data['Date'] == anomaly_date, 'Volume'].values[0]
        except (IndexError, KeyError):
            anomaly_volume_traded = float('nan')  # Handle cases where volume data might be missing

        try:
            last_volume_traded = stock_data.loc[stock_data['Date'] < anomaly_date].iloc[-1]['Volume']
            volume_change = ((anomaly_volume_traded - last_volume_traded) / last_volume_traded) * 100
        except (IndexError, KeyError):
            volume_change = float('nan')  # Handle cases where volume data might be missing

        # Retrieve closing prices and calculate price change
        try:
            last_close_price = stock_data.loc[stock_data['Date'] < anomaly_date].iloc[-1]['Close']
            anomaly_close_price = stock_data.loc[stock_data['Date'] == anomaly_date, 'Close'].values[0]
            price_change = ((anomaly_close_price - last_close_price) / last_close_price) * 100
        except (IndexError, KeyError):
            price_change = float('nan')  # Handle cases where price data might be missing

        # Calculate S&P 500 Price Change %
        try:
            sp500_anomaly_close_price = sp500_data.loc[sp500_data['Date'] == anomaly_date, 'Close'].values[0]
            sp500_last_close_price = sp500_data.loc[sp500_data['Date'] < anomaly_date].iloc[-1]['Close']
            sp500_price_change = ((sp500_anomaly_close_price.item() - sp500_last_close_price.item()) / sp500_last_close_price.item()) * 100
            sp500_price_change = round(sp500_price_change, 3)
        except (IndexError, KeyError):
            sp500_price_change = 'N/A'

        # Calculate Comparative Performance
        try:
            comparative_performance = ((price_change - sp500_price_change) / sp500_price_change) * 100
            comparative_performance = round(comparative_performance, 1)
        except:
            comparative_performance = 'N/A'

        # Ensure values are formatted correctly
        anomaly_close_price = round(float(anomaly_close_price), 2) if not pd.isna(anomaly_close_price) else 'N/A'
        price_change = round(float(price_change), 3) if not pd.isna(price_change) else 'N/A'
        volume_change = round(float(volume_change), 3) if not pd.isna(volume_change) else 'N/A'

        table_html += f"<tr><td style='text-align: center;'>{date_str}</td><td style='text-align: center;'>{anomaly_close_price}</td><td style='text-align: center;'>{volume_change}</td><td style='text-align: center;'>{price_change}</td><td style='text-align: center;'>{sp500_price_change}</td><td style='text-align: center;'>{comparative_performance}</td></tr>"

    table_html += "</table></div>"
    return table_html

# Function to fetch and plot stock data
def fetch_and_plot_stock_data(stock_symbol):
    try:
        stock_data = fetch_stock_data(stock_symbol)  # Fetch the stock data for the given symbol
        stock_data = detect_anomalies(stock_data)  # Detect anomalies in the stock data
        
        fig, ax = plt.subplots(figsize=(12, 7))  # Create a new figure for plotting
        ax.plot(pd.to_datetime(stock_data['Date']), stock_data['Close'], label='Stock Price')  # Plot the stock price

        anomalies = stock_data[stock_data['Anomaly'] == -1]  # Identify anomalies
        
        peaks, _ = find_peaks(stock_data['Close'].values)  # Identify peaks in the stock price
        valleys, _ = find_peaks(-stock_data['Close'].values)  # Identify valleys in the stock price
        
        ax.scatter(pd.to_datetime(anomalies['Date']), anomalies['Close'], color='red', label='Anomalies')  # Plot anomalies
        ax.scatter(pd.to_datetime(stock_data.iloc[peaks]['Date']), stock_data.iloc[peaks]['Close'], marker='x', color='green', label='Peaks')  # Plot peaks
        ax.scatter(pd.to_datetime(stock_data.iloc[valleys]['Date']), stock_data.iloc[valleys]['Close'], marker='x', color='purple', label='Valleys')  # Plot valleys
        
        # Add vertical lines for anomalies, peaks, and valleys
        for anomaly_date in pd.to_datetime(anomalies['Date']):
            ax.axvline(x=anomaly_date, color='red', linestyle='--', linewidth=1)
        for peak_date in pd.to_datetime(stock_data.iloc[peaks]['Date']):
            ax.axvline(x=peak_date, color='green', linestyle='--', linewidth=1)
        for valley_date in pd.to_datetime(stock_data.iloc[valleys]['Date']):
            ax.axvline(x=valley_date, color='purple', linestyle='--', linewidth=1)

        # Customize plot appearance
        ax.legend(fontsize=12, loc='best', frameon=True, facecolor='white', title='Legend', title_fontproperties={'weight':'bold', 'size':12})
        ax.set_title(f'Stock Price and Detected Anomalies for {stock_symbol} Last 3 Months', fontsize=20, fontweight='bold')
        ax.set_xlabel('Date', fontsize=14, fontweight='bold')
        ax.set_ylabel('Price (USD)', fontsize=14, fontweight='bold')
        
        # Set y-axis intervals to 5
        ax.yaxis.set_major_locator(plt.MultipleLocator(5))
        
        # Add horizontal dashed lines for major grid lines
        ax.yaxis.grid(True, linestyle='--', linewidth=0.5)
        
        # Add major and minor ticks for the x-axis
        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))
        ax.xaxis.set_minor_locator(mdates.DayLocator(interval=1))
        ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d-%Y'))
        
        # Ensure all anomaly dates, peaks, and valleys are shown as x-ticks
        all_dates = pd.to_datetime(anomalies['Date']).tolist() + pd.to_datetime(stock_data.iloc[peaks]['Date']).tolist() + pd.to_datetime(stock_data.iloc[valleys]['Date']).tolist()
        ax.set_xticks(sorted(all_dates))
        
        plt.xticks(rotation=90, fontsize=10)  # Reduce the font size of the x-tick labels
        plt.tight_layout()
        
        return fig, generate_anomaly_table(stock_data, stock_symbol)  # Return the plot and anomaly table
    except Exception as e:
        print(f"Error fetching or plotting stock data: {str(e)}")  # Handle any errors
        return None, f"Error: {str(e)}"

#Implements a Gradio interface for uploading documents, chat, and streaming chat with privacy settings.
#Adds a section for stock anomaly detection and visualization.
#Ensures Gradio closes correctly and handles port conflicts.

# Gradio interface setup
with gr.Blocks() as demo:
    with gr.Row():
        gr.Markdown("<h1 style='margin-bottom: 5px;'>Stock Price Anomaly Detection Application (v1.Beta) - Created by Jamie Kraus - Updated 11.10.2024</h1>")
    with gr.Row():
        with gr.Column(scale=1):
            stock_symbol = gr.Textbox(label="Enter Stock Symbol", placeholder="e.g., NVDA", lines=1)
        with gr.Column(scale=1):
            fetch_btn = gr.Button("Fetch Stock Data")

    with gr.Row():
        with gr.Column(scale=1):
            file_input = gr.File(label="Select files to load", file_count="multiple")
            load_btn = gr.Button("Load Documents")
            load_status = gr.Textbox(label="Load Status")
            docs_output = gr.HTML(label="Current Loaded Documents")
            docs_btn = gr.Button("Clear Loaded Documents")
        with gr.Column(scale=2):
            plot_output = gr.Plot()
            anomaly_table = gr.HTML(label="Detected Anomalies")
        with gr.Column(scale=1):
            msg = gr.Textbox(label="Enter your question (shift+enter to send)", lines=3, placeholder="Type your question here...")
            chatbot = gr.Chatbot()
            clear = gr.Button("Clear")

    fetch_btn.click(fetch_and_plot_stock_data, inputs=[stock_symbol], outputs=[plot_output, anomaly_table])

    load_btn.click(document_loader.load_documents, inputs=[file_input], outputs=[load_status, docs_output], show_progress="hidden")
    docs_btn.click(document_loader.clear_loaded_documents, inputs=[], outputs=[load_status, docs_output])
    demo.load(document_loader.clear_loaded_documents, outputs=[docs_output])
    msg.submit(stream_response, inputs=[msg, chatbot], outputs=[chatbot])
    msg.submit(lambda: "", outputs=[msg])
    clear.click(lambda: None, None, chatbot, queue=False)

# Ensure Gradio closes correctly
def shutdown_gradio():
    global demo
    if demo:
        demo.close()
        demo = None  # Explicitly setting demo to None

os.environ["GRADIO_SERVER_PORT"] = "8081"  # Set the correct port
atexit.register(shutdown_gradio)  # Ensure Gradio shuts down properly

try:
    demo.launch(share=False, server_name="127.0.0.1", server_port=8081)  # Using port 8081 and binding to localhost
except OSError:
    print("Port 8081 is in use. Trying a different port...")
    demo.launch(share=False, server_name="127.0.0.1", server_port=None)  # Automatically find an available port

from IPython.display import display, HTML
display(HTML("<style>.output_png {display: none;}</style>"))
