   "source": [
    "# Stock Anomaly Detection Application (v1.Beta) - NVIDIA Developer and LlamaIndex Contest 2024 Submission - By Jamie Kraus Updated 11.10.2024\n",
    "\n",
    "################################################## PROJECT CODE STARTS ##############################################################\n",
    "# Includes all necessary imports for various functionalities such as handling PDFs, plotting, and interacting with the Qdrant vector database and NVIDIA API.\n",
    "# Initializes the NVIDIA API client and sets the API key.\n",
    "# Defines a class to handle vector storage and search operations using Qdrant, including methods for inserting and searching vectors.\n",
    "# Provides a helper function to get completions from the NVIDIA Nemotron model.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pdfplumber\n",
    "import gradio as gr\n",
    "from pydantic import BaseModel, Field\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as rest\n",
    "import traceback\n",
    "import requests\n",
    "import atexit\n",
    "import yfinance as yf\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "import pandas as pd\n",
    "import uuid\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import time\n",
    "import re\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "# Corrected imports from LlamaIndex based on confirmed paths\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext, Document\n",
    "from llama_index.embeddings.nvidia import NVIDIAEmbedding\n",
    "from llama_index.llms.nvidia import NVIDIA\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "class CustomQdrantVectorStore:\n",
    "    def __init__(self, url, port, collection_name, vector_size, distance, overwrite=False):\n",
    "        self.client = QdrantClient(url=url, port=port)\n",
    "        self.collection_name = collection_name\n",
    "\n",
    "        # Check if the collection already exists\n",
    "        try:\n",
    "            if self.client.get_collection(collection_name=self.collection_name):\n",
    "                if overwrite:\n",
    "                    self.client.delete_collection(collection_name=self.collection_name)\n",
    "                    self.client.create_collection(\n",
    "                        collection_name=self.collection_name,\n",
    "                        vectors_config=rest.VectorParams(\n",
    "                            size=vector_size,\n",
    "                            distance=distance\n",
    "                        )\n",
    "                    )\n",
    "        except Exception as e:\n",
    "            print(f\"Error handling collection: {str(e)}\")\n",
    "\n",
    "    def insert(self, vectors):\n",
    "        try:\n",
    "            payload = [{'id': str(uuid.uuid4()), 'vector': vec.tolist()} for vec in vectors]\n",
    "            self.client.upsert(collection_name=self.collection_name, points=payload)\n",
    "        except Exception as e:\n",
    "            print(f\"Error inserting vectors: {str(e)}\")\n",
    "\n",
    "    def search(self, vector, top_k):\n",
    "        try:\n",
    "            results = self.client.search(\n",
    "                collection_name=self.collection_name,\n",
    "                query_vector=vector.tolist(),\n",
    "                limit=top_k\n",
    "            )\n",
    "            return results\n",
    "        except Exception as e:\n",
    "            print(f\"Error during search: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "# Initialize NVIDIA API client\n",
    "client = OpenAI(\n",
    "    api_key=\"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\",\n",
    "    base_url=\"https://integrate.api.nvidia.com/v1\"\n",
    ")\n",
    "\n",
    "# Function to get a completion from NVIDIA Nemotron\n",
    "def get_completion(system_content, user_content):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"nvidia/llama-3.1-nemotron-51b-instruct\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_content},\n",
    "            {\"role\": \"user\", \"content\": user_content}\n",
    "        ],\n",
    "        temperature=0.5,\n",
    "        top_p=1,\n",
    "        max_tokens=512,\n",
    "        stream=False\n",
    "    )\n",
    "    return completion.choices[0].message.content  # Adjusted to match the expected response format\n",
    "\n",
    "# Ensure NVIDIA API key is set\n",
    "os.environ[\"NVIDIA_API_KEY\"] = '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'\n",
    "\n",
    "#Defines the CustomDocument class for handling document data.\n",
    "#Implements utility functions for extracting text from PDFs, getting file names, and embedding texts using the NVIDIA model.\n",
    "\n",
    "# Define CustomDocument class\n",
    "class CustomDocument(BaseModel):\n",
    "    text: str\n",
    "    id_: str\n",
    "    unique_hash: int\n",
    "\n",
    "    @staticmethod\n",
    "    def create(text: str, id_: str):\n",
    "        unique_hash = hash(text)\n",
    "        return CustomDocument(text=text, id_=id_, unique_hash=unique_hash)\n",
    "\n",
    "# Extract text from PDF\n",
    "def extract_text_from_pdf(file_path):\n",
    "    try:\n",
    "        with pdfplumber.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf.pages:\n",
    "                text += page.extract_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to get file names from file objects\n",
    "def get_files_from_input(file_objs):\n",
    "    if not file_objs:\n",
    "        return []\n",
    "    return [file_obj.name for file_obj in file_objs]\n",
    "\n",
    "# Function to embed texts using NVIDIA model\n",
    "def embed_texts_with_nvidia(texts):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        if len(text) > 511:\n",
    "            text = text[:511]  # Truncate the text to stay within the maximum allowed token size\n",
    "        response = client.embeddings.create(\n",
    "            input=[text],\n",
    "            model=\"nvidia/nv-embedqa-mistral-7b-v2\",\n",
    "            encoding_format=\"float\",\n",
    "            extra_body={\"input_type\": \"query\", \"truncate\": \"NONE\"}\n",
    "        )\n",
    "        # Truncate the embedding to the first 256 dimensions to fit Qdrant's expected dimension\n",
    "        truncated_embedding = response.data[0].embedding[:256]\n",
    "        embeddings.append(truncated_embedding)\n",
    "    return np.array(embeddings)\n",
    "\n",
    "# Defines the DocumentLoader class for handling the loading of documents,\n",
    "# checking for duplicates, extracting text, and embedding texts using the CustomQdrantVectorStore\n",
    "\n",
    "class DocumentLoader:\n",
    "    def __init__(self):\n",
    "        self.loaded_documents = set()\n",
    "        self.qdrant_url = \"http://localhost\"\n",
    "        self.collection_name = \"qdrant-server\"\n",
    "        self.vector_store = CustomQdrantVectorStore(\n",
    "            url=self.qdrant_url,\n",
    "            port=6333,\n",
    "            collection_name=self.collection_name,\n",
    "            vector_size=256,\n",
    "            distance='Cosine',\n",
    "            overwrite=False\n",
    "        )\n",
    "\n",
    "    def load_documents(self, file_input):\n",
    "        if file_input is None:\n",
    "            return \"No documents selected.\", self.generate_document_table()\n",
    "        \n",
    "        existing_titles = {os.path.basename(doc) for doc in self.loaded_documents}\n",
    "        load_status = \"\"\n",
    "        documents = []\n",
    "        duplicates_found = False\n",
    "        \n",
    "        for file in file_input:\n",
    "            if os.path.basename(file.name) in existing_titles:\n",
    "                duplicates_found = True\n",
    "                continue\n",
    "            text = extract_text_from_pdf(file.name)\n",
    "            if text is None:\n",
    "                return f\"Error extracting text from file {file.name}.\", self.generate_document_table()\n",
    "            doc = CustomDocument.create(text=text, id_=file.name)\n",
    "            documents.append(doc)\n",
    "            full_file_path = os.path.abspath(file.name)\n",
    "            self.loaded_documents.add(full_file_path)\n",
    "        \n",
    "        if duplicates_found:\n",
    "            return \"Already uploaded PDF(s), please upload a new PDF.\", self.generate_document_table()\n",
    "\n",
    "        if not documents:\n",
    "            return \"No documents found in the selected files.\", self.generate_document_table()\n",
    "\n",
    "        texts = [doc.text for doc in documents]\n",
    "        embeddings = embed_texts_with_nvidia_parallel(texts)\n",
    "        points = [\n",
    "            rest.PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector=embedding.tolist(),\n",
    "                payload={\"file_path\": full_file_path, \"doc_id\": doc.id_}\n",
    "            )\n",
    "            for doc, embedding, file in zip(documents, embeddings, file_input)\n",
    "        ]\n",
    "        \n",
    "        self.vector_store.client.upsert(\n",
    "            collection_name=self.collection_name,\n",
    "            points=points\n",
    "        )\n",
    "        load_status = f\"Successfully loaded {len(documents)} documents.\"\n",
    "        return load_status, self.generate_document_table()\n",
    "    \n",
    "    def generate_document_table(self):\n",
    "        table_html = \"\"\"\n",
    "        <div style='height: 300px; overflow-y: auto; border: 1px solid #ccc; padding: 10px;'>\n",
    "            <table style='table-layout: fixed; width: 100%;'>\n",
    "                <tr>\n",
    "                    <th style='text-align: center;width: 50px;'>No.</th>\n",
    "                    <th style='text-align: center;width: 92px;'>File Directory</th>\n",
    "                    <th style='text-align: center;width: 92px;'>PDF Title</th>\n",
    "                    <th style='text-align: center;width: 60px;'>Pages</th>\n",
    "                </tr>\n",
    "        \"\"\"\n",
    "        total_documents = len(self.loaded_documents)\n",
    "        for index, doc_path in enumerate(self.loaded_documents, 1):\n",
    "            pdf_title = os.path.basename(doc_path)\n",
    "            short_doc_path = (doc_path if len(doc_path) <= 15 else doc_path[:15] + '... <button onclick=\"this.previousSibling.textContent=\\'' + doc_path + '\\'; this.textContent=\\'See Less\\'; this.onclick=function(){this.previousSibling.textContent=\\'' + doc_path[:15] + '... \\'; this.textContent=\\'See More\\';}\">See More</button>')\n",
    "            try:\n",
    "                with pdfplumber.open(doc_path) as pdf:\n",
    "                    num_pages = len(pdf.pages)\n",
    "                table_html += f\"<tr><td style='text-align: center; width: 30px;'>{index}</td><td style='word-wrap: break-word; overflow-x: auto; text-align: center;'>{short_doc_path}</td><td style='word-wrap: break-word; overflow-x: auto; text-align: center;'>{pdf_title}</td><td style='word-wrap: break-word; overflow-x: auto; text-align: center;'>{num_pages}</td></tr>\"\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing PDF {pdf_title}: {e}\")\n",
    "                table_html += f\"<tr><td style='text-align: center; width: 30px;'>{index}</td><td style='word-wrap: break-word; overflow-x: auto; text-align: center;'>{short_doc_path}</td><td style='word-wrap: break-word; overflow-x: auto; text-align: center;'>{pdf_title}</td><td style='word-wrap: break-word; overflow-x: auto; text-align: center;'>Error</td></tr>\"\n",
    "        table_html += \"</table></div>\"\n",
    "        return table_html + f\"<p style='text-align: right;'>Total Documents: {total_documents}</p>\"\n",
    "    \n",
    "    def clear_loaded_documents(self):\n",
    "        self.loaded_documents.clear()\n",
    "        return \"Cleared loaded documents.\", self.generate_document_table()\n",
    "\n",
    "document_loader = DocumentLoader()\n",
    "\n",
    "# Implements the chat function for handling chat interactions using the CustomQdrantVectorStore.\n",
    "# Implements the stream_response function for streaming responses, combining LLM responses with document information from Qdrant.\n",
    "\n",
    "# Function to summarize text using NVIDIA Nemotron LLM\n",
    "def summarize_text(text):\n",
    "    system_prompt = \"Please summarize the following text briefly and concisely answering the user prompt question and be conversational:\"\n",
    "    user_prompt = text\n",
    "    summary = get_completion(system_prompt, user_prompt)\n",
    "    return summary\n",
    "\n",
    "# Function to extract relevant snippets from PDF based on search results\n",
    "def extract_relevant_snippets(query_text):\n",
    "    search_result = document_loader.vector_store.client.search(\n",
    "        collection_name=document_loader.collection_name,\n",
    "        query_vector=embed_texts_with_nvidia([query_text])[0].tolist(),  # Embed the query text\n",
    "        limit=5\n",
    "    )\n",
    "    snippets = []\n",
    "    processed_files = set()\n",
    "    for result in search_result:\n",
    "        doc_id = result.payload[\"doc_id\"]\n",
    "        if doc_id in processed_files:\n",
    "            continue  # Skip duplicate files\n",
    "        text = extract_text_from_pdf(doc_id)\n",
    "        # Use only relevant segments of the text\n",
    "        snippet_start = text.lower().find(query_text.lower())\n",
    "        if snippet_start != -1:\n",
    "            snippet_end = snippet_start + min(500, len(text) - snippet_start)  # Extract up to 500 characters around the query\n",
    "            snippets.append(text[snippet_start:snippet_end])\n",
    "        else:\n",
    "            snippets.append(text[:500])  # Fallback to first 500 characters\n",
    "        processed_files.add(doc_id)\n",
    "    combined_snippets = \" \".join(snippets)\n",
    "    return combined_snippets\n",
    "\n",
    "# Function to retrieve and summarize text from the vector database\n",
    "def retrieve_and_summarize(query_text):\n",
    "    combined_snippets = extract_relevant_snippets(query_text)\n",
    "    summary = summarize_text(combined_snippets)\n",
    "    return summary\n",
    "\n",
    "# Function to generate responses using NVIDIA Nemotron LLM with inference\n",
    "def generate_response_with_inference(query, context):\n",
    "    system_prompt = f\"Given the following context from relevant documents, please answer the user's query concisely and correctly. If you don't find an exact match, provide relevant information that might be useful:\\n{context}\"\n",
    "    user_prompt = f\"{query}\"\n",
    "    response = get_completion(system_prompt, user_prompt)\n",
    "    return response\n",
    "\n",
    "# Function to handle chat interactions\n",
    "def chat(message, history):\n",
    "    try:\n",
    "        if not message.strip():\n",
    "            raise ValueError(\"The input message is empty or too short\")\n",
    "        \n",
    "        # Embed the query\n",
    "        query_embedding = embed_texts_with_nvidia([message])[0]\n",
    "        if len(query_embedding) != 256:\n",
    "            raise ValueError(f\"Vector dimension error: expected dim: 256, got {len(query_embedding)}\")\n",
    "        \n",
    "        # Search for similar embeddings in Qdrant\n",
    "        search_result = document_loader.vector_store.client.search(\n",
    "            collection_name=document_loader.collection_name,\n",
    "            query_vector=query_embedding.tolist(),\n",
    "            limit=5\n",
    "        )\n",
    "        \n",
    "        # Fetch the relevant documents and extract the text\n",
    "        responses = []\n",
    "        for result in search_result:\n",
    "            doc_id = result.payload[\"doc_id\"]\n",
    "            text = extract_text_from_pdf(doc_id)\n",
    "            if text:\n",
    "                responses.append(text)\n",
    "        \n",
    "        # Combine the retrieved texts for inference\n",
    "        combined_text = \" \".join(responses)\n",
    "        if not combined_text.strip():\n",
    "            combined_text = \"You asked: \" + message\n",
    "        \n",
    "        # Use the LLM to generate an inference based on the retrieved text or fallback message\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"nvidia/llama-3.1-nemotron-51b-instruct\",\n",
    "            messages=[{\"role\": \"system\", \"content\": combined_text}, {\"role\": \"user\", \"content\": message}],\n",
    "            temperature=0.5,\n",
    "            top_p=1,\n",
    "            max_tokens=512,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        response = \"\"\n",
    "        for chunk in completion:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                response += chunk.choices[0].delta.content\n",
    "        \n",
    "        return history + [(message, response)]\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_message = f\"Error processing query: {str(e)}\\n{traceback.format_exc()}\"\n",
    "        print(error_message)\n",
    "        return history + [(message, error_message)]\n",
    "\n",
    "# Function to stream responses\n",
    "max_retries = 3\n",
    "retry_delay = 5\n",
    "\n",
    "def stream_response(message, history):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            if not message.strip():\n",
    "                raise ValueError(\"The input message is empty or too short\")\n",
    "            \n",
    "            # Embed the query\n",
    "            query_embedding = embed_texts_with_nvidia_parallel([message])[0]\n",
    "            if len(query_embedding) != 256:\n",
    "                raise ValueError(f\"Vector dimension error: expected dim: 256, got {len(query_embedding)}\")\n",
    "            \n",
    "            # Search for similar embeddings in Qdrant\n",
    "            search_result = document_loader.vector_store.client.search(\n",
    "                collection_name=document_loader.collection_name,\n",
    "                query_vector=query_embedding.tolist(),\n",
    "                limit=5\n",
    "            )\n",
    "            \n",
    "            # Fetch the relevant documents and extract the text\n",
    "            responses = []\n",
    "            for result in search_result:\n",
    "                doc_id = result.payload[\"doc_id\"]\n",
    "                text = extract_text_from_pdf(doc_id)\n",
    "                if text:\n",
    "                    responses.append(text)\n",
    "            \n",
    "            # Combine the retrieved texts for inference\n",
    "            combined_text = \" \".join(responses[:2])  # Limit to the first 2 documents to speed up processing\n",
    "            if not combined_text.strip():\n",
    "                combined_text = \"You asked: \" + message\n",
    "            \n",
    "            # Increase timeout for the API request\n",
    "            openai.api_timeout = 60  # 1 minute timeout\n",
    "            \n",
    "            # Use the LLM to generate an inference based on the retrieved text or fallback message\n",
    "            completion = client.chat.completions.create(\n",
    "                model=\"nvidia/llama-3.1-nemotron-51b-instruct\",\n",
    "                messages=[{\"role\": \"system\", \"content\": combined_text}, {\"role\": \"user\", \"content\": message}],\n",
    "                temperature=0.5,\n",
    "                top_p=1,\n",
    "                max_tokens=512,\n",
    "                stream=True\n",
    "            )\n",
    "            \n",
    "            partial_response = \"\"\n",
    "            for chunk in completion:\n",
    "                if chunk.choices[0].delta.content is not None:\n",
    "                    partial_response += chunk.choices[0].delta.content\n",
    "                    yield history + [(message, partial_response)]\n",
    "            break  # Exit the retry loop if successful\n",
    "        \n",
    "        except openai.APITimeoutError:\n",
    "            if attempt < max_retries - 1:\n",
    "                time.sleep(retry_delay)\n",
    "            else:\n",
    "                error_message = \"Error processing query: Request timed out after multiple attempts.\"\n",
    "                print(error_message)\n",
    "                yield history + [(message, error_message)]\n",
    "                break\n",
    "        except Exception as e:\n",
    "            error_message = f\"Error processing query: {str(e)}\\n{traceback.format_exc()}\"\n",
    "            print(error_message)\n",
    "            yield history + [(message, error_message)]\n",
    "            break\n",
    "\n",
    "# Functions to fetch stock data and detect anomalies using an Isolation Forest algorithm.\n",
    "# Functions to plot stock data with detected anomalies and generate an HTML table of anomalies.\n",
    "\n",
    "# Function to fetch stock data from Yahoo Finance\n",
    "def fetch_stock_data(symbol):\n",
    "    stock_data = yf.download(tickers=symbol, period=\"3mo\", interval=\"1d\")  # Fetch stock data for the last 3 months with daily interval\n",
    "    if stock_data.empty:\n",
    "        raise ValueError(f\"No data found for symbol {symbol}\")\n",
    "    stock_data.reset_index(inplace=True)\n",
    "    # Flatten the multi-index DataFrame\n",
    "    stock_data.columns = [' '.join(col).strip() if isinstance(col, tuple) else col for col in stock_data.columns]\n",
    "    # Rename columns dynamically\n",
    "    stock_data.columns = [col.replace(f\" {symbol}\", \"\") for col in stock_data.columns]\n",
    "    print(f\"Fetched data: \\n{stock_data.tail()}\")  # Debugging information\n",
    "    return stock_data\n",
    "\n",
    "# Function to detect anomalies in the stock data using Isolation Forest\n",
    "def detect_anomalies(stock_data):\n",
    "    if 'Close' not in stock_data.columns or stock_data.empty:\n",
    "        raise ValueError(\"Stock data is not in the expected format or is empty.\")\n",
    "    \n",
    "    isolation_forest = IsolationForest(n_estimators=500, max_samples=0.95, contamination=0.45, random_state=42)\n",
    "    stock_data['Anomaly'] = isolation_forest.fit_predict(stock_data[['Close']])\n",
    "    stock_data['Anomaly'] = stock_data['Anomaly'].astype(int)\n",
    "    print(f\"Detected anomalies: \\n{stock_data[['Date', 'Close', 'Anomaly']].tail()}\")  # Debugging information\n",
    "    \n",
    "    # Ensure the first and last rows are not marked as anomalies\n",
    "    if 'Anomaly' in stock_data.columns and not stock_data.empty:\n",
    "        stock_data.at[stock_data.index[0], 'Anomaly'] = 1 if stock_data.at[stock_data.index[0], 'Anomaly'] == -1 else stock_data.at[stock_data.index[0], 'Anomaly']\n",
    "        stock_data.at[stock_data.index[-1], 'Anomaly'] = 1 if stock_data.at[stock_data.index[-1], 'Anomaly'] == -1 else stock_data.at[stock_data.index[-1], 'Anomaly']\n",
    "    \n",
    "    return stock_data\n",
    "\n",
    "# Function to fetch and plot stock data with detected anomalies\n",
    "def fetch_and_plot_stock_data(stock_symbol):\n",
    "    try:\n",
    "        stock_data = fetch_stock_data(stock_symbol)  # Fetch the stock data\n",
    "        stock_data = detect_anomalies(stock_data)  # Detect anomalies in the stock data\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        ax.plot(pd.to_datetime(stock_data['Date']), stock_data['Close'], label='Stock Price')  # Plot the stock price\n",
    "        \n",
    "        anomalies = stock_data[stock_data['Anomaly'] == -1]\n",
    "        print(f\"Anomalies data: \\n{anomalies}\")  # Debugging information\n",
    "        \n",
    "        peaks, _ = find_peaks(stock_data['Close'].values)  # Find peaks in the stock price\n",
    "        valleys, _ = find_peaks(-stock_data['Close'].values)  # Find valleys in the stock price\n",
    "        \n",
    "        # Plot anomalies, peaks, and valleys\n",
    "        ax.scatter(pd.to_datetime(anomalies['Date']), anomalies['Close'], color='red', label='Anomalies')\n",
    "        ax.scatter(pd.to_datetime(stock_data.iloc[peaks]['Date']), stock_data.iloc[peaks]['Close'], marker='x', color='green', label='Peaks')\n",
    "        ax.scatter(pd.to_datetime(stock_data.iloc[valleys]['Date']), stock_data.iloc[valleys]['Close'], marker='x', color='purple', label='Valleys')\n",
    "        \n",
    "        ax.legend()\n",
    "        ax.set_title(f'Stock Price and Detected Anomalies for {stock_symbol}')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Price')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return fig, generate_anomaly_table(stock_data, stock_symbol)  # Return the plot and anomaly table\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching or plotting stock data: {str(e)}\")\n",
    "        return None, f\"Error: {str(e)}\"\n",
    "\n",
    "# Function to load index data from Yahoo Finance\n",
    "def load_index_data(index_symbol):\n",
    "    data = yf.download(index_symbol, period=\"1y\")  # Load one year's data for safety\n",
    "    data.reset_index(inplace=True)  # Ensure the date is included in the DataFrame\n",
    "    return data\n",
    "\n",
    "# Function to get comparative analysis with major indices\n",
    "def get_comparative_analysis(stock_data, date_str, index_symbols):\n",
    "    \"\"\"Perform comparative analysis with major indices.\"\"\"\n",
    "    \n",
    "    stock_change = calculate_price_change(stock_data, pd.to_datetime(date_str))  # Calculate price change for the stock\n",
    "    analysis_results = []\n",
    "    \n",
    "    for index_name, symbol in index_symbols.items():\n",
    "        index_data = load_index_data(symbol)  # Load index data\n",
    "        index_change = calculate_price_change(index_data, pd.to_datetime(date_str))  # Calculate price change for the index\n",
    "        \n",
    "        if np.isnan(stock_change) or np.isnan(index_change):\n",
    "            analysis_result = f\"{index_name}: N/A\"\n",
    "        else:\n",
    "            analysis_result = f\"{index_name}: {(((stock_change - index_change) / index_change) * 100):.2f}%\"  # Comparative performance\n",
    "        analysis_results.append(analysis_result)\n",
    "    \n",
    "    return f\"Stock: {stock_change if not np.isnan(stock_change) else 'N/A'}% \" + \" \".join(analysis_results)\n",
    "\n",
    "# Updated function for calculate_price_change to handle missing dates\n",
    "def calculate_price_change(data, date):\n",
    "    try:\n",
    "        if date not in data['Date'].values:\n",
    "            return float('nan')  # Return NaN if the date is not in the data\n",
    "        idx = data.index[data['Date'] == date].tolist()[0]\n",
    "        if idx == 0:\n",
    "            return 0.0  # Return 0.0 if the date is the first entry in the data\n",
    "        prev_close = data.iloc[idx - 1]['Close']  # Get the previous day's closing price\n",
    "        curr_close = data.iloc[idx]['Close']  # Get the current day's closing price\n",
    "        price_change = ((curr_close - prev_close) / prev_close) * 100  # Calculate the percentage price change\n",
    "        return round(price_change, 4)  # Return the price change rounded to 4 decimal places\n",
    "    except IndexError:\n",
    "        print(f\"Error: Date {date} not found in data or data is not properly indexed.\")  # Handle errors if the date is not found\n",
    "        return float('nan')  # Return NaN in case of an error\n",
    "\n",
    "# Function to generate an HTML table of anomalies with comparative performance\n",
    "def generate_anomaly_table(stock_data, stock_symbol):\n",
    "    # Convert 'Date' column to datetime without timezone\n",
    "    stock_data['Date'] = pd.to_datetime(stock_data['Date']).dt.tz_localize(None)\n",
    "    \n",
    "    # Load S&P 500 data for comparison\n",
    "    sp500_data = yf.download('^GSPC', period=\"1y\")[['Close']]\n",
    "    sp500_data.reset_index(inplace=True)\n",
    "    sp500_data['Date'] = pd.to_datetime(sp500_data['Date']).dt.tz_localize(None)\n",
    "    sp500_data.sort_values(by='Date', inplace=True)  # Ensure the data is sorted by date\n",
    "\n",
    "    # Create HTML table for displaying anomalies and comparisons\n",
    "    table_html = f\"\"\"\n",
    "    <div style='height: 300px; overflow-y: auto; border: 1px solid #ccc; padding: 10px;'>\n",
    "        <table style='table-layout: fixed; width: 100%;'>\n",
    "            <tr>\n",
    "                <th style='text-align: center;width: 100px;'>{stock_symbol} Anomaly Detected Date</th>\n",
    "                <th style='text-align: center;width: 70px;'>{stock_symbol} Close Price (USD)</th>\n",
    "                <th style='text-align: center;width: 70px;'>{stock_symbol} Volume Change (%)</th>\n",
    "                <th style='text-align: center;width: 70px;'>{stock_symbol} Price Change (%)</th>\n",
    "                <th style='text-align: center;width: 90px;'>S&P 500 (^GSPC) Price Change (%)</th>\n",
    "                <th style='text-align: center;width: 100px;'>Comparative Performance to ^GSPC (%)</th>\n",
    "            </tr>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract anomalies and generate comparisons\n",
    "    anomalies = stock_data[stock_data['Anomaly'] == -1]\n",
    "\n",
    "    for i, anomaly in enumerate(anomalies['Date']):\n",
    "        date_str = pd.to_datetime(anomaly).strftime(\"%m-%d-%Y\")  # Format date as MM-DD-YYYY\n",
    "        anomaly_date = pd.to_datetime(anomaly)\n",
    "        \n",
    "        # Retrieve anomaly volume traded and calculate volume change\n",
    "        try:\n",
    "            anomaly_volume_traded = stock_data.loc[stock_data['Date'] == anomaly_date, 'Volume'].values[0]\n",
    "        except (IndexError, KeyError):\n",
    "            anomaly_volume_traded = float('nan')  # Handle cases where volume data might be missing\n",
    "\n",
    "        try:\n",
    "            last_volume_traded = stock_data.loc[stock_data['Date'] < anomaly_date].iloc[-1]['Volume']\n",
    "            volume_change = ((anomaly_volume_traded - last_volume_traded) / last_volume_traded) * 100\n",
    "        except (IndexError, KeyError):\n",
    "            volume_change = float('nan')  # Handle cases where volume data might be missing\n",
    "\n",
    "        # Retrieve closing prices and calculate price change\n",
    "        try:\n",
    "            last_close_price = stock_data.loc[stock_data['Date'] < anomaly_date].iloc[-1]['Close']\n",
    "            anomaly_close_price = stock_data.loc[stock_data['Date'] == anomaly_date, 'Close'].values[0]\n",
    "            price_change = ((anomaly_close_price - last_close_price) / last_close_price) * 100\n",
    "        except (IndexError, KeyError):\n",
    "            price_change = float('nan')  # Handle cases where price data might be missing\n",
    "\n",
    "        # Calculate S&P 500 Price Change %\n",
    "        try:\n",
    "            sp500_anomaly_close_price = sp500_data.loc[sp500_data['Date'] == anomaly_date, 'Close'].values[0]\n",
    "            sp500_last_close_price = sp500_data.loc[sp500_data['Date'] < anomaly_date].iloc[-1]['Close']\n",
    "            sp500_price_change = ((sp500_anomaly_close_price.item() - sp500_last_close_price.item()) / sp500_last_close_price.item()) * 100\n",
    "            sp500_price_change = round(sp500_price_change, 3)\n",
    "        except (IndexError, KeyError):\n",
    "            sp500_price_change = 'N/A'\n",
    "\n",
    "        # Calculate Comparative Performance\n",
    "        try:\n",
    "            comparative_performance = ((price_change - sp500_price_change) / sp500_price_change) * 100\n",
    "            comparative_performance = round(comparative_performance, 1)\n",
    "        except:\n",
    "            comparative_performance = 'N/A'\n",
    "\n",
    "        # Ensure values are formatted correctly\n",
    "        anomaly_close_price = round(float(anomaly_close_price), 2) if not pd.isna(anomaly_close_price) else 'N/A'\n",
    "        price_change = round(float(price_change), 3) if not pd.isna(price_change) else 'N/A'\n",
    "        volume_change = round(float(volume_change), 3) if not pd.isna(volume_change) else 'N/A'\n",
    "\n",
    "        table_html += f\"<tr><td style='text-align: center;'>{date_str}</td><td style='text-align: center;'>{anomaly_close_price}</td><td style='text-align: center;'>{volume_change}</td><td style='text-align: center;'>{price_change}</td><td style='text-align: center;'>{sp500_price_change}</td><td style='text-align: center;'>{comparative_performance}</td></tr>\"\n",
    "\n",
    "    table_html += \"</table></div>\"\n",
    "    return table_html\n",
    "\n",
    "# Function to fetch and plot stock data\n",
    "def fetch_and_plot_stock_data(stock_symbol):\n",
    "    try:\n",
    "        stock_data = fetch_stock_data(stock_symbol)  # Fetch the stock data for the given symbol\n",
    "        stock_data = detect_anomalies(stock_data)  # Detect anomalies in the stock data\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 7))  # Create a new figure for plotting\n",
    "        ax.plot(pd.to_datetime(stock_data['Date']), stock_data['Close'], label='Stock Price')  # Plot the stock price\n",
    "\n",
    "        anomalies = stock_data[stock_data['Anomaly'] == -1]  # Identify anomalies\n",
    "        \n",
    "        peaks, _ = find_peaks(stock_data['Close'].values)  # Identify peaks in the stock price\n",
    "        valleys, _ = find_peaks(-stock_data['Close'].values)  # Identify valleys in the stock price\n",
    "        \n",
    "        ax.scatter(pd.to_datetime(anomalies['Date']), anomalies['Close'], color='red', label='Anomalies')  # Plot anomalies\n",
    "        ax.scatter(pd.to_datetime(stock_data.iloc[peaks]['Date']), stock_data.iloc[peaks]['Close'], marker='x', color='green', label='Peaks')  # Plot peaks\n",
    "        ax.scatter(pd.to_datetime(stock_data.iloc[valleys]['Date']), stock_data.iloc[valleys]['Close'], marker='x', color='purple', label='Valleys')  # Plot valleys\n",
    "        \n",
    "        # Add vertical lines for anomalies, peaks, and valleys\n",
    "        for anomaly_date in pd.to_datetime(anomalies['Date']):\n",
    "            ax.axvline(x=anomaly_date, color='red', linestyle='--', linewidth=1)\n",
    "        for peak_date in pd.to_datetime(stock_data.iloc[peaks]['Date']):\n",
    "            ax.axvline(x=peak_date, color='green', linestyle='--', linewidth=1)\n",
    "        for valley_date in pd.to_datetime(stock_data.iloc[valleys]['Date']):\n",
    "            ax.axvline(x=valley_date, color='purple', linestyle='--', linewidth=1)\n",
    "\n",
    "        # Customize plot appearance\n",
    "        ax.legend(fontsize=12, loc='best', frameon=True, facecolor='white', title='Legend', title_fontproperties={'weight':'bold', 'size':12})\n",
    "        ax.set_title(f'Stock Price and Detected Anomalies for {stock_symbol} Last 3 Months', fontsize=20, fontweight='bold')\n",
    "        ax.set_xlabel('Date', fontsize=14, fontweight='bold')\n",
    "        ax.set_ylabel('Price (USD)', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # Set y-axis intervals to 5\n",
    "        ax.yaxis.set_major_locator(plt.MultipleLocator(5))\n",
    "        \n",
    "        # Add horizontal dashed lines for major grid lines\n",
    "        ax.yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "        \n",
    "        # Add major and minor ticks for the x-axis\n",
    "        ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "        ax.xaxis.set_minor_locator(mdates.DayLocator(interval=1))\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%m-%d-%Y'))\n",
    "        \n",
    "        # Ensure all anomaly dates, peaks, and valleys are shown as x-ticks\n",
    "        all_dates = pd.to_datetime(anomalies['Date']).tolist() + pd.to_datetime(stock_data.iloc[peaks]['Date']).tolist() + pd.to_datetime(stock_data.iloc[valleys]['Date']).tolist()\n",
    "        ax.set_xticks(sorted(all_dates))\n",
    "        \n",
    "        plt.xticks(rotation=90, fontsize=10)  # Reduce the font size of the x-tick labels\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        return fig, generate_anomaly_table(stock_data, stock_symbol)  # Return the plot and anomaly table\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching or plotting stock data: {str(e)}\")  # Handle any errors\n",
    "        return None, f\"Error: {str(e)}\"\n",
    "\n",
    "#Implements a Gradio interface for uploading documents, chat, and streaming chat with privacy settings.\n",
    "#Adds a section for stock anomaly detection and visualization.\n",
    "#Ensures Gradio closes correctly and handles port conflicts.\n",
    "\n",
    "# Gradio interface setup\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Row():\n",
    "        gr.Markdown(\"<h1 style='margin-bottom: 5px;'>Stock Price Anomaly Detection Application (v1.Beta) - Created by Jamie Kraus - Updated 11.10.2024</h1>\")\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            stock_symbol = gr.Textbox(label=\"Enter Stock Symbol\", placeholder=\"e.g., NVDA\", lines=1)\n",
    "        with gr.Column(scale=1):\n",
    "            fetch_btn = gr.Button(\"Fetch Stock Data\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=1):\n",
    "            file_input = gr.File(label=\"Select files to load\", file_count=\"multiple\")\n",
    "            load_btn = gr.Button(\"Load Documents\")\n",
    "            load_status = gr.Textbox(label=\"Load Status\")\n",
    "            docs_output = gr.HTML(label=\"Current Loaded Documents\")\n",
    "            docs_btn = gr.Button(\"Clear Loaded Documents\")\n",
    "        with gr.Column(scale=2):\n",
    "            plot_output = gr.Plot()\n",
    "            anomaly_table = gr.HTML(label=\"Detected Anomalies\")\n",
    "        with gr.Column(scale=1):\n",
    "            msg = gr.Textbox(label=\"Enter your question (shift+enter to send)\", lines=3, placeholder=\"Type your question here...\")\n",
    "            chatbot = gr.Chatbot()\n",
    "            clear = gr.Button(\"Clear\")\n",
    "\n",
    "    fetch_btn.click(fetch_and_plot_stock_data, inputs=[stock_symbol], outputs=[plot_output, anomaly_table])\n",
    "\n",
    "    load_btn.click(document_loader.load_documents, inputs=[file_input], outputs=[load_status, docs_output], show_progress=\"hidden\")\n",
    "    docs_btn.click(document_loader.clear_loaded_documents, inputs=[], outputs=[load_status, docs_output])\n",
    "    demo.load(document_loader.clear_loaded_documents, outputs=[docs_output])\n",
    "    msg.submit(stream_response, inputs=[msg, chatbot], outputs=[chatbot])\n",
    "    msg.submit(lambda: \"\", outputs=[msg])\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "# Ensure Gradio closes correctly\n",
    "def shutdown_gradio():\n",
    "    global demo\n",
    "    if demo:\n",
    "        demo.close()\n",
    "        demo = None  # Explicitly setting demo to None\n",
    "\n",
    "os.environ[\"GRADIO_SERVER_PORT\"] = \"8081\"  # Set the correct port\n",
    "atexit.register(shutdown_gradio)  # Ensure Gradio shuts down properly\n",
    "\n",
    "try:\n",
    "    demo.launch(share=False, server_name=\"127.0.0.1\", server_port=8081)  # Using port 8081 and binding to localhost\n",
    "except OSError:\n",
    "    print(\"Port 8081 is in use. Trying a different port...\")\n",
    "    demo.launch(share=False, server_name=\"127.0.0.1\", server_port=None)  # Automatically find an available port\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.output_png {display: none;}</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9291865f-0aa1-4504-8122-d48e825d1355",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
